<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-04-02T18:07:29+11:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">A Mathematical Structure named “Blog”</title><subtitle>Hi! Welcome to my website. I am hoping to post content related to mathematics and statistics here during my studies and beyond.  Hosted on Github Pages using Jekyll. Mathematical equations are displayed with MathJax.</subtitle><entry><title type="html">Recurrence and Transience in Simple Random Walks (The Adventures of Drunk Ants, Men and Bees)</title><link href="http://localhost:4000/random/walks/2022/04/01/Random_Walk.html" rel="alternate" type="text/html" title="Recurrence and Transience in Simple Random Walks (The Adventures of Drunk Ants, Men and Bees)" /><published>2022-04-01T01:00:00+11:00</published><updated>2022-04-01T01:00:00+11:00</updated><id>http://localhost:4000/random/walks/2022/04/01/Random_Walk</id><content type="html" xml:base="http://localhost:4000/random/walks/2022/04/01/Random_Walk.html">&lt;p&gt;Recently (March/April 2022), my stochastic processes class started discussing Markov processes and the concept of recurrence and transience. Some of the most well-known results in this area concern simple random walks on \(\mathbb{Z}^d\) and their transience and recurrence properties depending on the dimensions \(d\) (I have seen these referred to as Pòlya’s Random Walk Theorems online). The results of these theorems admit some very intriguing, easily accessible analogies: “drunk ants and men will surely find their way home, but a drunk bird may get lost forever”. In honour of how catchy this analogy is, I decided to write up a post with some definitions, proofs and silly pictures. Let’s do it!&lt;/p&gt;

&lt;p&gt;We begin by defining a simple random walk. We will only concern ourselves with walks in time \(T = \{0,1,2, \dots\}\) and on the state space \(\mathbb{Z}^d\) for \(d \geq 1\).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Definition 1.&lt;/em&gt; A &lt;em&gt;simple random walk&lt;/em&gt; is a discrete stochastic process \((X_n)_{n \in T}\) on the state space \(\mathbb{Z}^d\) such that at every time step, exactly \(1\) of the coordinates changes by \(+1\) or \(-1\), with all such transitions occurring with equal probability.&lt;/p&gt;

&lt;p&gt;For example, the simple random walk with \(d = 1\) starting at \(0\) is a sequence that increases by \(1\) or decreases by \(1\) with probability \(1/2\) each. The simple random walk on \(\mathbb{Z}^2\) starting at \((0,0)\) has four possible positions to move to in the first timestep (these are \((0,1), (1,0), (-1,0), (0,-1)\)) and hence each transition occurs with probability \(1/4\), and so on.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Definition 2.&lt;/em&gt; A simple random walk on \(\mathbb{Z}^d\) is said to be &lt;em&gt;recurrent&lt;/em&gt; if, for any state \(x \in \mathbb{Z}^d\), the probability that the walk will return to \(x\) in finite time is equal to \(1\). Otherwise, it is said to be &lt;em&gt;transient&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Recurrent walks will always, at some point, find their way back to where they started. Conversely, transient walks have a positive probability of disappearing infinitely far away, never to return. We can write \(N_x\) to denote the random variable&lt;/p&gt;

\[N_x = \sum_{n=1}^\infty 1_{X_n}(x),\]

&lt;p&gt;which represents the number of times the random walk returns to \(x\). In this notation, the recurrence property can be written as the fact that there exists some state \(x\) such that&lt;/p&gt;

\[P(N_x \geq 1) = 1\]

&lt;p&gt;The &lt;em&gt;expected number of returns&lt;/em&gt; is given by the expectation&lt;/p&gt;

\[\mathbb{E}N_x = \sum_{n=1}^\infty P(X_n = x)\]

&lt;p&gt;Of course, we invoke Fubini’s theorem to justify the interchange of the infinite sum and expectation (I wouldn’t catch you doing this without justification, would I?). It can be shown that the expected number of returns \(\mathbb{E}N_x\) characterises recurrence in random walks (indeed, for Markov chains in general), with the following result:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proposition 1&lt;/em&gt; 
For simple random walks, we have the equivalence&lt;/p&gt;

\[\mathbb{E}N_x = \infty \text{ for any } x \in \mathbb{Z}^d \iff (X_n)_{n \in T} \text{ is recurrent.}\]

&lt;p&gt;&lt;em&gt;Proof&lt;/em&gt;:
Firstly, by the tail sum formula we can write the expected number of returns to \(x\) as the sum over the probabilities&lt;/p&gt;

\[\mathbb{E}N_x = \sum_{n \geq 1} P(N_x \geq n)\]

&lt;p&gt;where \(P(N_x \geq n)\) denotes the probability that the number of returns to \(x\) is greater than \(n\).&lt;/p&gt;

&lt;p&gt;Write \(T_{x,n}\) as the random variable denoting the duration of the \(n\)th “excursion” from \(x\) (that is, after returning to \(x\) \(n-1\) times, how long does it take the walk to return to \(x\) next?). Then each \(T_{x,n}\) is independent and follows the same distribution for all \(n\). The probability that the number of returns to \(x\) being greater than \(n\) is given by&lt;/p&gt;

\[P(N_x \geq n \mid X_0 = 0)  \\
= P(T_{x,1} &amp;lt; \infty, P_{x,2}&amp;lt; \infty , \dots, T_{x,n} &amp;lt; \infty \mid X_0 = 0) \\
= P(T_{x,1} &amp;lt; \infty \mid X_0 = x)^n,\]

&lt;p&gt;and so by the tail sum formula, we have&lt;/p&gt;

\[\mathbb{E}(N_x \mid X_0 = x) = \sum_{q \geq 1} P(T_{x,1} &amp;lt; \infty \mid X_0=x)^q.\]

&lt;p&gt;Of course, if \((X_n)_{n \in T}\) is recurrent, then \(P(T_{x,1} &amp;lt; \infty \mid X_0 = x&amp;gt;) = 1\), and so \(\mathbb{E}(N_x \mid X_0 = x) = \infty\). Conversely, if the walk is not recurrent, then \(P(T_{x,1} &amp;lt; \infty \mid X_0 = x) &amp;lt; 1\), and the sum in the formula above is just a convergent geometric series, which is finite. This finishes the proof.&lt;/p&gt;

&lt;p&gt;With this in hand, we turn to proving our main results of the day, for which we will need Stirling’s approximation:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Lemma 1&lt;/em&gt; (Stirling’s Approximation): For integer \(n \geq 1\), we have the approximation&lt;/p&gt;

\[n! \approx \sqrt{2\pi n}\left( \frac{n}{e}\right)^n.\]

&lt;p&gt;Notation-wise, we write \(a(n) \approx b(n)\) if \(\frac{a(n)}{b(n)} \to 1\) as \(n \to \infty\). Generally speaking, most operations that work with equalities work with \(\approx\), which allows us to abuse notation with regards to manipulating expressions with \(\approx\). If in doubt, always remember we could rewrite \(\approx\) with an equivalent \(\varepsilon\)-type argument (which would be needlessly formal and obscure the intuition). With these background results in hand, we turn to stating and proving our main theorems of the day…&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Theorem 1 (Drunk Ants and Men)&lt;/em&gt;: The simple random walk on \(\mathbb{Z}^d\) for \(d = 1, 2\) are recurrent.&lt;/p&gt;

&lt;p&gt;Here are two very relevant drawings before the proof:
&lt;img src=&quot;/assets/random_walk1d.png&quot; alt=&quot;drawing1&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A drunk ant finds his way home…&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/random_walk2d.png&quot; alt=&quot;drawing&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;…and so too do does the drunk man (possibly taking a while longer than the ant).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proof&lt;/em&gt;: We note that the simple random walk is symmetric in the sense that any state is equivalent to any other state, so we fix \(x = 0\) and show that \(x = 0\) is recurrent (the proof will then work for any arbitrary state; indeed, it can be shown that any state being recurrent implies all states are recurrent for &lt;em&gt;irreducible&lt;/em&gt; Markov chains, but we do not go into this). We prove the results separately for \(d = 1\) and \(d = 2\). Both involve exactly the same idea: do some counting, compute \(\mathbb{E}N_x\) (or, at least, show it must converge) and sprinkle in Stirling’s formula liberally.&lt;/p&gt;

&lt;p&gt;For both cases, the random walks are &lt;em&gt;periodic with period 2&lt;/em&gt; in the sense that making a return trip starting and ending at any given state only has positive probability for even steps: that is, \(p_{x,x}^{(2n)} &amp;gt; 0\), where \(p_{x,x}^{(2n)}\) denotes the probability of starting then returning to \(x\) after exactly \(2n\) time-steps. Hence, to find \(\mathbb{E}N_0 = \sum_{n=1}^\infty P(X_{2n} = 0) = \sum_{n=1}^\infty p_{0,0}^{(2n)}\), we simply need to compute \(p_{0,0}^{(2n)}\).&lt;/p&gt;

&lt;p&gt;\(d=1\): 
In order to return to \(0\) after \(2n\) steps, we need to take exactly \(n\) steps forward and \(n\) steps back. There are \(\binom{2n}{n}\) ways of doing this, with \(2^{2n}\) total possible paths (since at each time-step, there are \(2\) possible choices). This gives us&lt;/p&gt;

\[p_{0,0}^{(2n)} = \binom{2n}{n} \frac{1}{2^{2n}} \\
= \frac{1}{2^{2n}} \frac{(2n)!}{(n!)^2}.\]

&lt;p&gt;Using Stirling’s formula to approximate each factorial, we then have&lt;/p&gt;

\[\frac{(2n)!}{(n!)^2} = \frac{\sqrt{4n\pi}\left(\frac{2n}{e}\right)^{2n}}{2n\pi \left(\frac{n}{e}\right)^{2n}} \\
= \frac{2^{2n}}{\sqrt{n \pi}},\]

&lt;p&gt;and so&lt;/p&gt;

\[p_{0,0}^{(2n)} = \frac{1}{2^{2n}} \frac{2^{2n}}{\sqrt{n \pi}} = \frac{1}{\sqrt{n \pi}}.\]

&lt;p&gt;Then&lt;/p&gt;

\[\mathbb{E}N_x = \sum_{n=1}^\infty p_{0,0}^{(2n)} = \sum_{n=1}^\infty\frac{1}{\sqrt{n \pi}} = \infty,\]

&lt;p&gt;and so the \(d = 1\) simple walk is recurrent, as we wanted to show&lt;/p&gt;

&lt;p&gt;\(d = 2\): The proof method is the same. To return to the origin after exactly \(2n\) steps, the walk must balance every “up” step with a “down” step and a “left” step with a “right” step. If there were \(k\) up steps, and hence \(n-k\) left steps, there must be \(k\) down steps and \(n-k\) right steps. The number of ways of this occurring with \(k\) fixed is given by the multinomial&lt;/p&gt;

\[\binom{2n}{k,k,n-k,n-k} = \frac{(2n)!}{(k!)^2 ((n-k)!)^2},\]

&lt;p&gt;with the total number of paths in \(2n\) steps given by \(4^{2n}\) (since there are \(4\) possible directions to go to in each time-step). Hence, summing over all possible values of \(k\), we get&lt;/p&gt;

\[p_{0,0}^{(2n)} = \frac{1}{4^{2n}} \sum_{k=0}^n  \frac{(2n)!}{(k!)^2 ((n-k)!)^2.\]

&lt;p&gt;We can simplify the right-hand side as follows:&lt;/p&gt;

\[\frac{1}{4^{2n}} \sum_{k=0}^n  \frac{(2n)!}{(k!)^2 ((n-k)!)^2 = \frac{1}{4^{2n}} \sum_{n=1}^\infty  \frac{(2n)! (n!)^2}{(n!)^2(k!)^2 ((n-k)!)^2 \\
= \frac{1}{4^{2n}} \binom{2n}{n} \sum_{k=0}^n \frac{(n!)^2}{(k!)^2 ((n-k)!)^2},\]

&lt;p&gt;where the summation simplifies to \(\sum_{k=0}^n \binom{n}{k}^2 = \binom{2n}{n}\) (a slick way of seeing this is true is to do the following: pick \(n\) objects from \(2n\) total objects; this can be done in \(\binom{2n}{n}\) ways. Now split the \(2n\) objects into two groups of \(n\) each. We can pick the same \(n\) objects by picking \(i\) from the first group and \(n-i\) from the second group and varying over \(i\). Credits to this &lt;a href=&quot;https://math.stackexchange.com/questions/320348/inductive-proof-that-2n-choose-n-sumn-choose-i2&quot;&gt;post&lt;/a&gt; for this proof!). Continuing on, we get&lt;/p&gt;

\[p_{0,0}^{(2n)} = \frac{1}{4^{2n}} \binom{2n}{n}^2 = \left(\frac{1}{4^n} \binom{2n}{n}\right)^2,\]

&lt;p&gt;and we can use Stirling’s formula to expand the binomial:&lt;/p&gt;

\[\frac{1}{4^n} \binom{2n}{n}= \frac{1}{4^n} \left( \frac{(2n)!}{(n!)^2} \right) \approx \frac{1}{4^n} \frac{\sqrt{4\pi n} \left(\frac{2n}{e}\right)^{2n}}{2\pi n \left( \frac{n}{e} \right)^{2n}},\]

&lt;p&gt;which simplifies to \(\frac{1}{\sqrt{\pi n}}\). That is to say, we have&lt;/p&gt;

\[p_{0,0}^{(2n)} \approx \frac{1}{n \pi} \implies \mathbb{E}[N_0 |X_0 = 0] = \sum_{n=1}^\infty \frac{1}{n\pi} = \infty,\]

&lt;p&gt;and we conclude that the two-dimensional simple random walk is also recurrent, as we wanted.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Remark on Theorem 1&lt;/em&gt;: If we wanted to be &lt;em&gt;really&lt;/em&gt; mysterious, we could say that drunk ants and men will always return home because the infinite sum \(\sum_n \frac{1}{\sqrt{n}}\) and the harmonic series \(\sum_n \frac{1}{n}\) diverges. Of course, there is far more that goes into it than just these divergence results…&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Theorem 2 (Lost drunk bees)&lt;/em&gt;: The simple random walks on \(\mathbb{Z}^d\) for \(d \geq 3\) are transient. 
Unfortunately, I will not be providing a proof of this result just yet. You see, proving Theorem 2 is part of an assignment problem set in my stochastic processes course, so after that is completed, I’ll write something up here!&lt;/p&gt;

&lt;p&gt;In the meantime, here is a bee: 
&lt;img src=&quot;/assets/random_walk3d.png&quot; alt=&quot;drawing2&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;</content><author><name>Tony Wang</name></author><category term="Random" /><category term="Walks" /><summary type="html">Recently (March/April 2022), my stochastic processes class started discussing Markov processes and the concept of recurrence and transience. Some of the most well-known results in this area concern simple random walks on \(\mathbb{Z}^d\) and their transience and recurrence properties depending on the dimensions \(d\) (I have seen these referred to as Pòlya’s Random Walk Theorems online). The results of these theorems admit some very intriguing, easily accessible analogies: “drunk ants and men will surely find their way home, but a drunk bird may get lost forever”. In honour of how catchy this analogy is, I decided to write up a post with some definitions, proofs and silly pictures. Let’s do it!</summary></entry><entry><title type="html">Infinitely Recurring Events occur with Probability Zero</title><link href="http://localhost:4000/probability/2022/03/14/Borel-Cantelli.html" rel="alternate" type="text/html" title="Infinitely Recurring Events occur with Probability Zero" /><published>2022-03-14T01:00:00+11:00</published><updated>2022-03-14T01:00:00+11:00</updated><id>http://localhost:4000/probability/2022/03/14/Borel-Cantelli</id><content type="html" xml:base="http://localhost:4000/probability/2022/03/14/Borel-Cantelli.html">&lt;p&gt;In this post, we give short proofs for the Borel-Cantelli lemma, which succinctly states that events which reoccur infinitely often in a sequence of events must have probability zero, and one of its consequences: unlikely events under one probability measure is unlikely in any probability measure “dominated” by it. We make all of these concepts precise.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Notation&lt;/em&gt;: We work in an arbitrary measure space \((\Omega, \mathcal{F}, P)\) with \(\{A_n\}_{n \in \mathbb{N}}\) a sequence of measurable sets. While we interpret the measure \(P\) as a probability measure and measurable sets as “events” in the outcome space \(\Omega\), there is no explicit requirement that \(P(\Omega) = 1\) for the theory to work. The expectation \(\mathbb{E}\) refers to the integral operator&lt;/p&gt;

\[\mathbb{E}[f] = \int_\Omega f(\omega) \: dP(\omega).\]

&lt;p&gt;&lt;em&gt;Definition 1&lt;/em&gt;: 
The limit superior of a sequence of measurable sets \(\{A_n\}_{n \in \mathbb{N}}\) is the set&lt;/p&gt;

\[\limsup_{n \to \infty} A_n := \cap_{n=1}^\infty \cup_{k \geq n} A_k.\]

&lt;p&gt;Intuitively speaking, the limit superior contains the set of events \(A_n\) which occur infinitely often in the sequence \(\{A_k\}_{k \in \mathbb{N}}\) - indeed, any event which occurs finitely often can only be contained within a finite number of the unions \(\cup_{k \geq n} A_k\), and hence cannot be in the intersection of all of those unions. Going off of this intuition, probability theorists may also write this as \(A_n \text{ i.o}\), standing for “\(A_n\) infinitely often”.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Definition 2&lt;/em&gt;:
A measure \(P\) is said to dominate a measure \(Q\) (written \(P &amp;gt;&amp;gt; Q\)) if&lt;/p&gt;

\[P(A) = 0 \implies Q(A) = 0.\]

&lt;p&gt;&lt;em&gt;Lemma 1 (Borel-Cantelli Lemma)&lt;/em&gt;: 
Suppose \(\sum_{k=1}^n \mathbb{E}[A_n] &amp;lt; \infty\). Then&lt;/p&gt;

\[P(\limsup_{n \to \infty} A_n) := P(\cap_{n=1}^\infty \cup_{k \geq n}A_k) = 0.\]

&lt;p&gt;&lt;em&gt;Proof&lt;/em&gt;: Note that \(\cup_{k \geq n}A_k\) is a decreasing sequence of sets in \(n\), with \(\cup_{k \geq n} A_k \supset \cup_{k \geq m} A)k\) if \(n \geq m\). Hence, by the continuity of measures, we can pull out the limit:&lt;/p&gt;

\[P(\cap_{n=1}^\infty \cup_{k \geq n} A_k) = \lim_{n \to \infty} P(\cup_{k \geq n} A_k).\]

&lt;p&gt;Now, each for each \(n\), we have&lt;/p&gt;

\[P(\cup_{k \geq n} A_k) \leq \sum_{k=n}^\infty P(A_k)\]

&lt;p&gt;by the sub-additivity property of measures. Since the total sum \(\sum_{k=1}^\infty P(A_k)\) is assumed to be finite, the tail sums \(\sum_{k = n}^\infty P(A_k)\) must converge to zero as \(n \to \infty\). We conclude that&lt;/p&gt;

\[P(\limsup_{n \to \infty} A_n) = 0,\]

&lt;p&gt;as we wanted.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Corollary 1 (Unlikely Events are Equally as Unlikely in Dominated Measures)&lt;/em&gt;:&lt;/p&gt;

&lt;p&gt;Suppose \(Q\) is another measure on \(\mathcal{F}\) and \(P\) dominates \(Q\) (i.e., \(P &amp;gt;&amp;gt; Q\)). Then for all \(\varepsilon &amp;gt; 0\), there exists \(\delta &amp;gt; 0\) such that&lt;/p&gt;

\[P(A) &amp;lt; \delta \implies Q(A) &amp;lt; \varepsilon.\]

&lt;p&gt;We interpret this condition probabilistically as follows: unlikely events under \(P\) forces those same events to be unlikely under any dominated measure \(Q\). This makes sense, but how would we prove this?&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proof&lt;/em&gt;: We argue by contradiction. Suppose there exists some \(\varepsilon_0 &amp;gt; 0\) such that for all \(k \in \mathbb{N}\), we have a sequence of measurable sets \(A_k\) with&lt;/p&gt;

\[P(A_k) &amp;lt; 2^{-k} \quad \text{but} \quad Q(A_k) \geq \varepsilon_0 &amp;gt; 0.\]

&lt;p&gt;Define \(A := \limsup_{n \to \infty} A_n = \cap_{n=1}^\infty \cup_{k \geq n} A_k\). Then&lt;/p&gt;

\[Q(A) = Q(\limsup_{n \to \infty} A_n) \\
=^{(*)} \lim_{n \to \infty} Q(\cup_{k \geq n}A_k) \\
\geq^{(**)} Q(A_n) \quad \text{for all } n \in \mathbb{N}, \\
\geq \varepsilon_0 &amp;gt; 0,\]

&lt;p&gt;where in Step \((*)\) we used the continuity of the measure \(Q\) (again, the unions of \(A_k\) is a decreasing sequeunce), and in Step \((**)\) we the fact that every union \(\cup_{k \geq n} A_k\) contains \(A_n\). However, we must have \(P(A) = 0\) by the Borel-Cantelli lemma, since 
\(\sum_{k=1}^\infty P(A_k) &amp;lt; \sum_{k=1}^\infty 2^{-k} &amp;lt; 1 &amp;lt; \infty.\)&lt;/p&gt;

&lt;p&gt;This contradicts the fact that \(P &amp;gt;&amp;gt; Q\), since we have a set of measure zero under \(P\) that does not have measure zero under \(Q\). This completes the proof.&lt;/p&gt;

&lt;p&gt;That’s all for today’s post!&lt;/p&gt;</content><author><name>Tony Wang</name></author><category term="Probability" /><summary type="html">In this post, we give short proofs for the Borel-Cantelli lemma, which succinctly states that events which reoccur infinitely often in a sequence of events must have probability zero, and one of its consequences: unlikely events under one probability measure is unlikely in any probability measure “dominated” by it. We make all of these concepts precise.</summary></entry><entry><title type="html">Anomaly Detection Project: Completion!</title><link href="http://localhost:4000/anomaly/detection/2022/03/10/Anomaly_Detection.html" rel="alternate" type="text/html" title="Anomaly Detection Project: Completion!" /><published>2022-03-10T01:00:00+11:00</published><updated>2022-03-10T01:00:00+11:00</updated><id>http://localhost:4000/anomaly/detection/2022/03/10/Anomaly_Detection</id><content type="html" xml:base="http://localhost:4000/anomaly/detection/2022/03/10/Anomaly_Detection.html">&lt;p&gt;Recently, I had the pleasure of being part of the team that submitted our results and working prototypes for the “AI for Decision-Making” (Stage 1 Phase 2) project proposed by the &lt;a href=&quot;https://defenceinnovationnetwork.com/&quot;&gt;Defence Innovation Network&lt;/a&gt;, a University-Defence lead iniative to provide funding for projects with relevance to Australian Defence.&lt;/p&gt;

&lt;p&gt;You can find our work, with demonstrations, in our &lt;a href=&quot;https://github.com/sjmluo/Contextually_Situated_Anomaly_Detection&quot;&gt;repository&lt;/a&gt;. We also wrote up a series of blog posts, aimed at introducing people to our work without getting overly technical. You can find this hosted &lt;a href=&quot;https://sjmluo.github.io/anomaly/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the future, I will write up my learnings from this project and on anomaly detection as a whole, to both remember and showcase the work we had done and how it might be applied and extended. Stay tuned!&lt;/p&gt;

&lt;h1 id=&quot;images&quot;&gt;Images&lt;/h1&gt;
&lt;p&gt;Some cool images from our work!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/CAC_responses.jpg&quot; alt=&quot;TOP_PHOTO&quot; width=&quot;100%&quot; /&gt;
Figure 1: With the appropriate data transformations, we can convert incoming time series signals from a sensor array into an anomaly predictor, with each sensor producing each one of these curves. The peaks of the curves correspond to likely anomalous transitions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/combined_CAC_24.jpg&quot; alt=&quot;TOP_PHOTO&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2: By leveraging multiple sources of data (in our experiments, 24 sensors worth of data), we can use data averaging techniques to combine the information effectively and obtain a smooth anomaly predictor that is robust against noise.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/smoothed_CAC_anim.gif&quot; alt=&quot;TOP_PHOTO&quot; width=&quot;100%&quot; /&gt;
Figure 3: We generalised our methods to work in real time by transforming sensor data in batches as it arrives. Peaks correspond to detected anomalies. Red lines correspond to the theoretical positions where an anomalous transition has taken place.&lt;/p&gt;</content><author><name>Tony Wang</name></author><category term="Anomaly" /><category term="Detection" /><summary type="html">Recently, I had the pleasure of being part of the team that submitted our results and working prototypes for the “AI for Decision-Making” (Stage 1 Phase 2) project proposed by the Defence Innovation Network, a University-Defence lead iniative to provide funding for projects with relevance to Australian Defence.</summary></entry><entry><title type="html">The Basics of Measure-Theoretic Probability Theory</title><link href="http://localhost:4000/probability/2022/03/09/Measure-Theory.html" rel="alternate" type="text/html" title="The Basics of Measure-Theoretic Probability Theory" /><published>2022-03-09T01:44:00+11:00</published><updated>2022-03-09T01:44:00+11:00</updated><id>http://localhost:4000/probability/2022/03/09/Measure-Theory</id><content type="html" xml:base="http://localhost:4000/probability/2022/03/09/Measure-Theory.html">&lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt;
&lt;p&gt;As of 2022 Term 1, I am currently taking the course MATH5835, Advanced Stochastic Processes at USNW. It has been both an absolute joy and a struggle in the first four weeks of this course, often interchanging between the two. It is a joy because I had always wanted to go in-depth into measure-theoretic probability theory (there is a certain magical allure to saying something like, &lt;em&gt;the conditional expectation exists and is unique almost surely by the Radon-Nikodym Theorem&lt;/em&gt;…), and a struggle since everything is so shrouded in the depths of abstraction (I really had to think about it when a &lt;em&gt;stopping time&lt;/em&gt; was defined as a measurable map!). And, honestly speaking, when is learning anything ever not a struggle? This blog post, and everything after it, will follow my journey through this course and serve as my (public-facing) notebook, for jotting down thoughts, definitions and clarifications. Stay tuned!&lt;/p&gt;

&lt;h1 id=&quot;motivations&quot;&gt;Motivations&lt;/h1&gt;
&lt;p&gt;Everyone knows what &lt;em&gt;probability&lt;/em&gt; means, in some intuitive sense. Primary schoolers learn how there’s a 50% chance rain on any given day (okay - at the time of writing, a years worth of rain has fallen in many areas in Australia, two weeks in a row. Maybe that’s not quite true…), first year undergrads learn about coin-flipping experiments and the binomial distribution, somewhere along the way we become acquainted with other common distributions (for example, the normal, exponential, Poisson, Gamma, beta families, in no particular order) and how we can use them to create simple models for many real-world situations, and at some point we just cross our fingers and hope all of our sums and integrals converge and everything is well-behaved. After all, as statistics practitioners, we can just leave that to the theoreticians, right?&lt;/p&gt;

&lt;p&gt;It’s certainly a common, if not justified, viewpoint to take - after all, we only have a finite amount of time in a day, and we couldn’t learn everything there is to know in this world. Personally speaking, I think my time is well-served by digging deep into the foundations, the formalisms and definitions, the bounds and estimates - I don’t want to know which levers to pull to make things “work”, I want to know &lt;em&gt;why&lt;/em&gt; they work. So - here it is!&lt;/p&gt;

&lt;h1 id=&quot;the-beginning&quot;&gt;The Beginning&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;Definition 1&lt;/em&gt;: Let \(\Omega\) be a set. A \(\sigma\)-algebra \(\mathcal{F}\) on \(\Omega\) is a set of subsets of \(\Omega\) which satisfy the following properties:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;\(\mathcal{F}\) contains the total set \(\Omega\).&lt;/li&gt;
  &lt;li&gt;\(\mathcal{F}\) is closed under complements: for every set \(A \in \mathcal{F}\), the set \(A^c\) is in \(\mathcal{F}\).&lt;/li&gt;
  &lt;li&gt;\(\mathcal{F}\) is closed under countable unions: for sets \(A_i \in \mathcal{F}\), \(i \in I \subset \mathbb{N}\) with \(I\) a countable index set, we have \(\cup_{i \in I} A_i \in \mathcal{F}\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The \(\sigma\)-algebra is a key player in probability theory: intuitively speaking, it describes the set of events on which we can assign probabilities to (under this formalism, with very reasonable assumptions, we can indeed create “events” that cannot be assigned a probability! We call these “non-measurable” sets.). You can see it intuitively from the definition as well: if we know the probability of some event \(E\) occurring, then certainly we know the probability of \(E^c\) occurring. If we konw the probability of a (countable) number of events happening, then we could talk about the union of all these events occurring: that should have a well-defined probability as well.&lt;/p&gt;

&lt;p&gt;We also note that \(\sigma\)-algebras are also often called \(\sigma\)-fields in probability theory. We will use these terms interchangeably, although most often calling them \(\sigma\)-algebras (since this term applies in more contexts).&lt;/p&gt;

&lt;p&gt;The rest is under construction! Check back later! :)&lt;/p&gt;</content><author><name>Tony Wang</name></author><category term="Probability" /><summary type="html">Background As of 2022 Term 1, I am currently taking the course MATH5835, Advanced Stochastic Processes at USNW. It has been both an absolute joy and a struggle in the first four weeks of this course, often interchanging between the two. It is a joy because I had always wanted to go in-depth into measure-theoretic probability theory (there is a certain magical allure to saying something like, the conditional expectation exists and is unique almost surely by the Radon-Nikodym Theorem…), and a struggle since everything is so shrouded in the depths of abstraction (I really had to think about it when a stopping time was defined as a measurable map!). And, honestly speaking, when is learning anything ever not a struggle? This blog post, and everything after it, will follow my journey through this course and serve as my (public-facing) notebook, for jotting down thoughts, definitions and clarifications. Stay tuned!</summary></entry><entry><title type="html">Automatic Feature Extraction: An Introduction to Deep 1D Convolution Networks</title><link href="http://localhost:4000/machine/learning/2022/03/09/The_1D_Convolution.html" rel="alternate" type="text/html" title="Automatic Feature Extraction: An Introduction to Deep 1D Convolution Networks" /><published>2022-03-09T01:44:00+11:00</published><updated>2022-03-09T01:44:00+11:00</updated><id>http://localhost:4000/machine/learning/2022/03/09/The_1D_Convolution</id><content type="html" xml:base="http://localhost:4000/machine/learning/2022/03/09/The_1D_Convolution.html">&lt;h1 id=&quot;context&quot;&gt;Context&lt;/h1&gt;

&lt;p&gt;As a part of my work in the &lt;a href=&quot;https://twan3617.github.io/anomaly/detection/2022/03/09/Anomaly_Detection.html&quot;&gt;Anomaly Detection Project&lt;/a&gt;, I looked at ways of benchmarking our algorithms performance with standard anomaly detection algorithms in the sensor array literature. One particular model that struck me as interesting was the 1D convolution network (for literature references, see &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0022460X18301792&quot;&gt;1&lt;/a&gt; and &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0888327020307846&quot;&gt;2&lt;/a&gt;). I was already familiar with the widespread use of 2D convolutions for object detection in computer vision, but had never thought about how these convolutions might be used in time series analysis.&lt;/p&gt;

&lt;p&gt;Time series data is often some of the more difficult data to analyse from a statistical point of view. There is often a large quantity of data points; data compression is almost always necessary for computational power, and any model we use must take into account the autocorrelation (i.e. correlation in time) of the data. Feature extraction is a common first step in analysing time series; this involves compressing a given time series into a number of features which (hopefully) characterise the data without losing any information. Future analyses would focus on working with this set of features with typical statistical techniques such as clustering (k-means), machine learning algorithms (random forests, support vector machines, neural networks), regression and so on. However, this process is often fraught with danger. Features that return state-of-the-art results in one area often perform very poorly in another. For example, in our anomaly detection project, I tried to use state-of-the-art features for electroencephalogram (EEG) data classification (see &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S1746809420302299&quot;&gt;3&lt;/a&gt;) with a tuned SVM; in our (heavily noise-affected) domain, such features barely outperformed random guessing.&lt;/p&gt;

&lt;p&gt;This is where 1D convolution networks come in. 1D convolution networks remove the need for domain expertise and manpower spent on manual feature extraction. Feeding in raw time series data, 1D convolution networks process the data through multiple layers of “moving average windows” (filters) before combining the predictions in a final dense layer. By optimising the weights in each filter using standard backpropagation algorithms (ADAM), we can train a network to categorise time series data just as we do for images.&lt;/p&gt;

&lt;p&gt;Below, we discuss the theory in more depth, present some of my work in applying 1D convolution learning to supervised classification problems and evaluate their performance in the context of algorithm performance and applicability in anomaly detection.&lt;/p&gt;

&lt;h1 id=&quot;theory&quot;&gt;Theory&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/convolve_operation.png&quot; alt=&quot;TOP_PHOTO&quot; width=&quot;100%&quot; /&gt;
The basic operation we need to be familiar with is the “convolution” operation. Given a 1D block of data, we can convolve it with a filter (or window) of weights, which just means to take the dot product of the two vectors together to obtain a single number. That’s all there is to it!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/Time_Series_Filters.jpg&quot; alt=&quot;TOP_PHOTO&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;p&gt; We take a filter and slide it through the time series, taking the convolution at each step to produce a single number. The output of this filter will just be these numbers concatenated into a single vector, which produces another time series. Since this operation will reduce the size of the input, we can pad the original time series by adding in empty blocks (zeros) on the side so that the input and output time series are the same size.  &lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/assets/Time_Series_Multiple_Filters.png&quot; alt=&quot;TOP_PHOTO&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;p&gt; Now that we have convolution filters, we can build up a single convolution layer by stacking together multiple filters. Each filter has its own fixed set of weights independent of the weights of the other filters in its layer. The weights of each filter are optimised to capture some unique feature of the signal, analogous to how 2D convolution filters capture features such as edges in images. Each filter operates independently to the other filters and layers until the final dense layer.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/assets/1Dconvnet_architecture.jpg&quot; alt=&quot;TOP_PHOTO&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;p&gt;We use 1D max pooling layers in-between convolution layers to reduce computation complexity, and a final dense layer to combine all the features together. After the forward pass, we can evaluate the networks fitness using a loss function (usually mean square error or 0-1 loss), and use an optimisation algorithm to shift all the weights in each convolution layer in the direction that minimises this loss function. By stacking all of these layers together, we can create a deep end-to-end anomaly predictor which takes in raw data and outputs the most likely class directly. The image above is the network that was implemented in my project. &lt;/p&gt;

&lt;h1 id=&quot;data-description&quot;&gt;Data Description&lt;/h1&gt;
&lt;p&gt;I provide some implementations of 1D convolution networks in Tensorflow on a number of data sets. The first is the Sandia Laboratories Experimental Structure Data (the &lt;em&gt;bookshelf&lt;/em&gt; data set), and the second is the Airbus Helicopter Anomalies Data set (the &lt;em&gt;helicopter&lt;/em&gt; data set). In both cases, the goal is to categorise each time series into a class based on the amount of damage sustained by the structure. In the bookshelf data set, we are given the exact level of damage applied to the structure before the structure was shaken by a motor. In the helicopter data set, we are not given much information at all, aside from the true class of a given data point.&lt;/p&gt;

&lt;h2 id=&quot;bookshelf-data-set&quot;&gt;Bookshelf Data Set&lt;/h2&gt;
&lt;p&gt;The data description can be found &lt;a href=&quot;/assets/Bookshelf.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;helicopter-data-set&quot;&gt;Helicopter Data Set&lt;/h2&gt;
&lt;p&gt;See &lt;a href=&quot;https://www.research-collection.ethz.ch/handle/20.500.11850/415151&quot;&gt;here&lt;/a&gt; for the source of the data.&lt;/p&gt;
&lt;h1 id=&quot;implementation&quot;&gt;Implementation&lt;/h1&gt;
&lt;p&gt;Please see &lt;a href=&quot;https://github.com/twan3617/1D_conv_net_Analysis&quot;&gt;my GitHub repository here&lt;/a&gt; for the notebooks and code.&lt;/p&gt;</content><author><name>Tony Wang</name></author><category term="Machine" /><category term="Learning" /><summary type="html">Context</summary></entry><entry><title type="html">Common Distributions</title><link href="http://localhost:4000/probability/2022/03/09/List_of_Distributions.html" rel="alternate" type="text/html" title="Common Distributions" /><published>2022-03-09T01:44:00+11:00</published><updated>2022-03-09T01:44:00+11:00</updated><id>http://localhost:4000/probability/2022/03/09/List_of_Distributions</id><content type="html" xml:base="http://localhost:4000/probability/2022/03/09/List_of_Distributions.html">&lt;p&gt;In this post, I will write about some of the key distributions that are used by statisticians. The aim is to not just have the formulas, but explain intuition, proofs and uses for these distributions. These will be added in as my time permits. I will mainly be using this page as a reference.&lt;/p&gt;

&lt;p&gt;Each distribution will have its probability mass function (pmf) or probability density function (pdf) listed, mean and variance, and some notes on relations to other distributions and derivations.&lt;/p&gt;

&lt;h1 id=&quot;measure-theoretic-view&quot;&gt;Measure-Theoretic View&lt;/h1&gt;
&lt;p&gt;Suppose we are working with a process that has sample space \(\Omega\), set of measurable events \(\mathcal{F}\) and probability measure \(P\). A random variable \(X\) (taking values in \(\overline{\mathbb{R}} = \mathbb{R} \cup \{\infty\}\)) is just a measurable function \(X: \Omega \to \overline{\mathbb{R}}\). The expectation (or &lt;em&gt;mean&lt;/em&gt;) of \(X\) is the (Lebesgue) integral 
\(\int_\Omega f \: dP,\) 
defined in the usual way (this is discussed in a separate blog post). The &lt;em&gt;probability distribution&lt;/em&gt; of \(X\) is the function \(P \circ X^{-1} : \overline{\mathbb{R}} \to [0,1]\) (with respect to the usual Borel \(\sigma\)-algebra). The probability distribution describes how likely the random variable \(X\) is to take on some set of values. The &lt;em&gt;distribution function&lt;/em&gt; (or &lt;em&gt;cumulative distribution function&lt;/em&gt;) of \(X\) is defined as \(F(x) := $(P \circ X^{-1})((-\infty, x]))\), the probability that \(X\) takes on values up to and including \(x\).&lt;/p&gt;

&lt;p&gt;Finally, a &lt;em&gt;probability density&lt;/em&gt; of \(X\) on \(\mathbb{R}\) is defined as follows. For a non-negative measurable \(f \geq 0\) on \((\Omega, \mathcal{F}, P)\), we can write \(f \cdot P\) to define a new measure \(\mu\) by&lt;/p&gt;

&lt;p&gt;\(\mu (A) = (f \cdot P) (A) = \int_A f \: dP \quad \text{for all } A \in \mathcal{F}.\)
We call \(f\) the \(P\)-density of \(\mu\). If the function \(f\) is the density of \(P \circ X^{-1}\) with respect to the Lebesgue measure on \(\mathbb{R}\), then \(f\) is called the &lt;em&gt;probability density function&lt;/em&gt; of \(X\).&lt;/p&gt;

&lt;h1 id=&quot;notation-and-abbreviations&quot;&gt;Notation and Abbreviations&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;X: generic random variable.&lt;/li&gt;
  &lt;li&gt;pmf: probability mass function. pdf: probability distribution function.&lt;/li&gt;
  &lt;li&gt;mgf: moment generating function, defined by \(m(t) = \mathbb{E}\exp(tX)\).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;discrete-probability-distributions&quot;&gt;Discrete Probability Distributions&lt;/h2&gt;

&lt;h1 id=&quot;bernoulli-binomial-and-multinomial-distributions&quot;&gt;Bernoulli, Binomial and Multinomial Distributions&lt;/h1&gt;
&lt;p&gt;Bernoulli\((p)\): The Bernoulli random variable takes values \(1\) with probability \(p\) and \(0\) with probability \(1-p\) for \(0 \leq p \leq 1\). The probability of getting \(x\) for \(x = 0\) or \(x = 1\) is given by&lt;/p&gt;

\[P(X=x|p) = p^x (1-p)^{1-x}.\]

&lt;p&gt;Mean: \(\mathbb{E}X = p\). Variance: \(\text{Var} X = p(1-p)\).
mgf: \(M_X(t) = (1-p) + p\exp(t)\).&lt;/p&gt;

&lt;p&gt;Binomial\((n,p)\): The binomial random variable is the sum of \(n\) Bernoulli random variables. By the Binomial theorem, we have&lt;/p&gt;

\[P(X=x|n,p) = {n \choose x} p^x (1-p)^{1-x} \quad \text{for } x = 0, 1, \dots, n \text{ and } 0 \leq p \leq 1.\]

&lt;p&gt;Mean: \(\mathbb{E}X = np\). Variance: \(\text{Var}X = np(1-p)\).
mgf: \(M_X(t) = (p\exp(t) + (1-p))^n\).&lt;/p&gt;

&lt;p&gt;Multinomial\((n, p_1, p_2, \dots, p_k)\): The multinomial distribution is a generalisation of the binomial random variable. Suppose we have \(k\) distinct events, each occurring with probability \(p_i\), and we have \(n\) total experiments. The multinomial coefficient is defined by&lt;/p&gt;

\[{n \choose k_1, k_2, \dots, k_n} = \frac{n!}{k_1! k_2! \dots k_n!} \quad \text{with } \sum_{i=1}^n k_i = n.\]

&lt;p&gt;Write \(X_i\) for the random variable representing the number of times event \(i\) occurs in \(n\) experiments and write \(X = (X_1, X_2, \dots, X_n)\). Then the pmf of \(X\) is given by&lt;/p&gt;

\[P(X=(x_1, x_2,\dots, x_k)|k,n,p) = {n \choose x_1, x_2, \dots, x_k} p_1^{x_1} p_2^{x_2} \dots p_k^{x_k}.\]

&lt;p&gt;We must have \(\sum_{i=1}^k x_k = n\) and \(\sum_{i=1}^k p_k = 1\).&lt;/p&gt;

&lt;p&gt;Mean: \(\mathbb{E}X_i = np_i\). \(\text{Var}X = np_i(1-p_i)\).
mgf: \((\sum_{i=1}^k p_i \exp(t_i))^n\).&lt;/p&gt;
&lt;h1 id=&quot;geometric-distribution&quot;&gt;Geometric Distribution&lt;/h1&gt;
&lt;p&gt;Suppose we are doing an experiment to see the number of binomial trials required to see one success, with \(p\) being the probability of success. The probability that the first success occurs on experiment \(x\) is given by the probability of failing \(x-1\) times and succeeding on the \(x\)th experiment:&lt;/p&gt;

\[P(X=x | p) = p(1-p)^{x-1}, \quad x = 1, 2, \dots; \quad 0 \leq p \leq 1.\]

&lt;p&gt;Mean: \(\mathbb{E}X = 1/p\). Var\(X = \frac{1-p}{p^2}.\)
mgf: \(M_X(t) = \frac{p\exp(t)}{1-(1-p)\exp(t)}, \quad t &amp;lt; -\log(1-p)\).&lt;/p&gt;

&lt;p&gt;The geometric distribution is memoryless, in the sense that \(P(X &amp;gt; s | X &amp;gt; t) = P(X &amp;gt; s-t)\): given that we know the success must occur after \(t\) experiments, the probability of \(X\) occuring after \(s\) experiments (where \(s &amp;gt; t\)) is exactly as if we had started the experiments from 0: that is, it is equal to the probability that \(X &amp;gt; s - t\).&lt;/p&gt;
&lt;h1 id=&quot;hypergeometric-distribution&quot;&gt;Hypergeometric Distribution&lt;/h1&gt;
&lt;p&gt;Suppose there are a total of \(M\) tagged fish in a total population of \(N\) fishes. Suppose further we have trawled up \(K\) fishes (with \(K \leq N\)). Then the probability of getting exactly \(x\) tagged fish is given by&lt;/p&gt;

\[P(X = x | N, M, K) = \frac{ \binom{M}{x}\binom{N-M}{K - x}}{\binom{N}{K}}\]

&lt;p&gt;This represents the probability of getting \(x\) of the fish from the tagged population \(M\), the remaining \(K - x\) fish from the non-tagged \(N-M\) population, divided by the total number of ways of getting \(K\) fish from \(N\) total fishes.&lt;/p&gt;

&lt;p&gt;Mean: \(\mathbb{E}X = KM/N\). Var\(X = \frac{KM}{N} \frac{(N-M)(N-K)}{N(N-1)}.\)&lt;/p&gt;
&lt;h1 id=&quot;poisson-distribution&quot;&gt;Poisson Distribution&lt;/h1&gt;
&lt;p&gt;Suppose some set of discrete events occur at an average fixed rate \(\lambda\), with no two events occurring at the same instantaneous moment and each event occurring independently of the other. Then the number of such events \(X\) which occur over a fixed period of time follows a Poisson distribution, with&lt;/p&gt;

\[P(X= x | \lambda) = \frac{\exp(-\lambda) \lambda^x}{x!}, \quad x = 0,1 , \dots; \quad 0 \leq \lambda &amp;lt; \infty.\]

&lt;p&gt;Mean: \(\mathbb{E}X = \lambda\). Var\((X) = \lambda\).
mgf: \(M_X(t) = \exp(\lambda(\exp^t-1))\).&lt;/p&gt;

&lt;h2 id=&quot;continuous-probability-distributions&quot;&gt;Continuous Probability Distributions&lt;/h2&gt;

&lt;h1 id=&quot;normal-distribution&quot;&gt;Normal Distribution&lt;/h1&gt;

&lt;h1 id=&quot;beta-distribution&quot;&gt;Beta Distribution&lt;/h1&gt;

&lt;h1 id=&quot;laplace-distribution&quot;&gt;Laplace Distribution&lt;/h1&gt;

&lt;h1 id=&quot;exponential-distribution&quot;&gt;Exponential Distribution&lt;/h1&gt;

&lt;h1 id=&quot;gamma-distribution&quot;&gt;Gamma Distribution&lt;/h1&gt;</content><author><name>Tony Wang</name></author><category term="Probability" /><summary type="html">In this post, I will write about some of the key distributions that are used by statisticians. The aim is to not just have the formulas, but explain intuition, proofs and uses for these distributions. These will be added in as my time permits. I will mainly be using this page as a reference.</summary></entry><entry><title type="html">The Mysteries of MathJax: Macros and Numbering</title><link href="http://localhost:4000/blog/hosting/2021/09/16/MathJax.html" rel="alternate" type="text/html" title="The Mysteries of MathJax: Macros and Numbering" /><published>2021-09-16T23:34:48+10:00</published><updated>2021-09-16T23:34:48+10:00</updated><id>http://localhost:4000/blog/hosting/2021/09/16/MathJax</id><content type="html" xml:base="http://localhost:4000/blog/hosting/2021/09/16/MathJax.html">&lt;h1 id=&quot;site-setup&quot;&gt;Site Setup&lt;/h1&gt;
&lt;p&gt;I decided to write this post as a reminder to future-me (and anyone else who might read this) about how this site was set up, and in particular, how to get MathJax working. This was a lengthy process, synthesising material from old 2012 StackExchange posts  (all slightly - but not completely - obsolete now) and dead-ends, and I don’t want to repeat the process whenever I come back to work on this site. If you know JavaScript/HTML/Ruby already, the process would probably much easier, but I don’t, so here goes!&lt;/p&gt;

&lt;h2 id=&quot;system-requirements&quot;&gt;System Requirements&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Currently working on MacOS Monterey 12.0.1, although this entire process works for at least MacOS 11.X. I believe Windows is not (officially) supported by Jekyll currently, although it could still be made to run without many tweaks.&lt;/li&gt;
  &lt;li&gt;MathJax 3.XX (Importantly, the syntax changed between MathJax 2.XX and 3.XX. If you are just getting started, this shouldn’t be important.)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;basics&quot;&gt;Basics&lt;/h2&gt;
&lt;p&gt;This site is set up using GitHub Pages, so all the assets are contained within a GitHub repository (see https://github.com/twan3617/twan3617.github.io for the assets running this blog). The code that makes this page work is based on Jekyll, and the basic introduction can be found here: https://docs.github.com/en/pages/setting-up-a-github-pages-site-with-jekyll.&lt;/p&gt;

&lt;p&gt;I will assume you have followed the tutorials and managed to set up Jekyll with a  theme that you like (for example, I’m using Minima). If you use the pre-set themes, your file structure should be exactly the same (if not, at least similar).&lt;/p&gt;

&lt;p&gt;There were some “gotchas” that took ages for me to figure out during the whole process. I will add them in as I remember them:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;When installing (or updating) Ruby/Jekyll, if your system is throwing errors about not being able to find the installation &lt;em&gt;despite it being right in front of your eyes&lt;/em&gt;, you may need to alter your PATH variable (if the installation is not in PATH, then the OS will never be able to find the right folders).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For example, let’s suppose I am installing Ruby 3.0.0. Since I am using zsh, I would go to ~/.zshrc and, with my favourite text editor (struggling to learn how to use vim!), paste in&lt;/p&gt;

\[\text{export PATH=&quot;/usr/local/opt/ruby/bin:/usr/local/lib/ruby/gems/3.0.0/bin:\$PATH&quot;}\]

&lt;p&gt;at the end of the file.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Use bundle exec jekyll serve to start up the server: find your website at localhost:4000.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;method&quot;&gt;Method&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Create mathjax.html inside your_site/_includes with the following code:&lt;/li&gt;
&lt;/ol&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;script&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;text/javascript&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;MathJax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;tex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;tags&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;ams&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;packages&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;base&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;ams&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;newcommand&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;configmacros&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;macros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;RR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;mathbb{R}}&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;ddx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;frac{d#2}{d#1}&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;ui/menu&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;[tex]/ams&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;sr&quot;&gt;/script&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&amp;gt;
&lt;/span&gt;  
&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;script&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;text/javascript&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;MathJax-script&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;async&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;sr&quot;&gt;/script&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The code with the URL&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;script&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;text/javascript&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;MathJax-script&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;async&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;sr&quot;&gt;/script&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;tells your server where to find the MathJax3 scripts. The code starting with window.MathJax is where you define your configurations. In particular, “tags: ‘ams’” numbers all equations in a specific equation format (you can use ‘all’ as well, which just numbers everything), “packages: …” is where you include TeX packages (importantly, in order for macros to run, you must include configmacros in packages), and finally, “macros: …”, where you can define custom macros that can be read. Macros are in the form&lt;/p&gt;

\[\text{command: &apos;TeX output&apos;}\]

&lt;p&gt;so in the code above, typing \RR in an equation setting would produce the math blackboard font \(\RR\).&lt;/p&gt;

&lt;p&gt;If you &lt;em&gt;don’t&lt;/em&gt; want MathJax to run on every page, you can include an if statement in the scripts as follows:&lt;/p&gt;

\[\{\% \text{  if page.mathjax = true  }  \%\}\]

\[\text{code script from above}\]

\[\{\% \text{  endif  } \%\}\]

&lt;p&gt;In the front matter of your html files (that is, the code between the triple dashes), include the line&lt;/p&gt;

\[\text{mathjax: true}\]

&lt;p&gt;Finally, under your_site/default.html, include the line&lt;/p&gt;

\[\text{\{\% include mathjax.html \%\}}\]

&lt;p&gt;in-between the HTML tags, so that every page which uses the default layout can read the code in mathjax.html. This should allow equations to be rendered!&lt;/p&gt;

&lt;h1 id=&quot;testing-equation-numbering-and-macros&quot;&gt;Testing Equation Numbering and Macros&lt;/h1&gt;
&lt;p&gt;Note that for in-line equations, unlike LaTeX, you need to type two dollar signs in order for your markdown reader to render the equation. If you want to type in an equation format, you should put the dollar signs on a new line.&lt;/p&gt;

&lt;p&gt;Below, we write an equation, give it a numbering, then reference the equation later.
\begin{equation}
\int_{\Omega} A(x, u, \nabla u) \phi \: dx = 0,    \label{123}
\end{equation}
In equation \eqref{123}, the equality holds for all \(\phi \in C_c^\infty(\Omega)\) (the set of compactly supported infinitely differentiable functions in \(\Omega \subseteq \RR\)).&lt;/p&gt;

&lt;p&gt;We have also defined macros that replace \RR with \(\RR\) and \ddx{f(x)} with \(\ddx{f(x)}\):
\begin{align}
\RR \quad \ddx{f(x)}
\end{align}&lt;/p&gt;

&lt;p&gt;As an aside, there is also a code highlighter: given a language, Jekyll can parse and highlight keywords in the code and return a nicely formatted block:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;To get this, simply type&lt;/p&gt;

&lt;p&gt;% highlight language %&lt;/p&gt;

&lt;p&gt;print(“hello world”)&lt;/p&gt;

&lt;p&gt;% endhighlight %&lt;/p&gt;

&lt;p&gt;while enclosing the top and bottom lines in braces {}.&lt;/p&gt;</content><author><name>Tony Wang</name></author><category term="Blog" /><category term="Hosting" /><summary type="html">Site Setup I decided to write this post as a reminder to future-me (and anyone else who might read this) about how this site was set up, and in particular, how to get MathJax working. This was a lengthy process, synthesising material from old 2012 StackExchange posts (all slightly - but not completely - obsolete now) and dead-ends, and I don’t want to repeat the process whenever I come back to work on this site. If you know JavaScript/HTML/Ruby already, the process would probably much easier, but I don’t, so here goes!</summary></entry><entry><title type="html">Thesis Ramblings: PDE and Analysis</title><link href="http://localhost:4000/mathematics/2021/09/16/Thesis.html" rel="alternate" type="text/html" title="Thesis Ramblings: PDE and Analysis" /><published>2021-09-16T20:11:00+10:00</published><updated>2021-09-16T20:11:00+10:00</updated><id>http://localhost:4000/mathematics/2021/09/16/Thesis</id><content type="html" xml:base="http://localhost:4000/mathematics/2021/09/16/Thesis.html">&lt;p&gt;Whenever one is asked about their background and their studies, one always has to keep in mind their audience to decide how much to &lt;em&gt;really&lt;/em&gt; say. As much fun as it might be spouting off all kinds of technicalities and making &lt;em&gt;absolutely sure&lt;/em&gt; they won’t ask you about what you do again, it’s also important to be aware that not everyone has the necessary background, training, pre-requisites, or even just interest (just asking to be polite, man!) This is certainly not a bad thing - after all, our world can only be so delightfully varied because different people specialise in different things. It just makes things so much more interesting when one actually meets someone in the same field, and can go beyond superficialities. Like realising that one person shares one of your main hobbies, and suddenly they’re “in the know” in this secret world of yours (any tennis or bouldering fans around? Nadal is my top man, and I can climb pinks on a good day - sounds like absolute gibberish if you’re into none of these things!).&lt;/p&gt;

&lt;p&gt;Nowadays, since I’m doing my Masters and focusing on statistics, it’s not too difficult to sprinkle some common lexicon words (data, statistics, machine learning, coding)…&lt;/p&gt;

&lt;p&gt;The rest is under construction! Check back later :)&lt;/p&gt;</content><author><name>Tony Wang</name></author><category term="Mathematics" /><summary type="html">Whenever one is asked about their background and their studies, one always has to keep in mind their audience to decide how much to really say. As much fun as it might be spouting off all kinds of technicalities and making absolutely sure they won’t ask you about what you do again, it’s also important to be aware that not everyone has the necessary background, training, pre-requisites, or even just interest (just asking to be polite, man!) This is certainly not a bad thing - after all, our world can only be so delightfully varied because different people specialise in different things. It just makes things so much more interesting when one actually meets someone in the same field, and can go beyond superficialities. Like realising that one person shares one of your main hobbies, and suddenly they’re “in the know” in this secret world of yours (any tennis or bouldering fans around? Nadal is my top man, and I can climb pinks on a good day - sounds like absolute gibberish if you’re into none of these things!).</summary></entry></feed>